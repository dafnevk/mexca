{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyannote.core import Annotation\n",
    "from mexca.text.transcription import AudioTextIntegrator, AudioTranscriber\n",
    "from mexca.audio.speaker_id import SpeakerIdentifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\n",
    "            'tests', 'reference_files', 'transcription_dutch_1_second.json'), 'r') as file:\n",
    "        transcription = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\n",
    "            'tests', 'reference_files', 'reference_audio_5_seconds.json'), 'r') as file:\n",
    "        ref_speakers = Annotation.from_json(json.loads(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(transcription, ref_speakers):\n",
    "    text, char_starts, char_ends = transcription['transcription'], transcription['start_timestamps'], transcription['end_timestamps']\n",
    "    \n",
    "    output = {'speech_start':[],'speech_end':[],'speaker':[],'text':[]}\n",
    "    for speech_turn, _, speaker in ref_speakers.itertracks(yield_label=True):\n",
    "        speech_start, speech_end = 1000 * round(speech_turn.start,3), 1000 * round(speech_turn.end,3) #convert to ms\n",
    "        extracted_text = ''.join([char  for i,char in enumerate(text) if (char_ends[i]>=speech_start and char_starts[i]<=speech_end)])\n",
    "        output['speech_start'].append(speech_start)\n",
    "        output['speech_end'].append(speech_end)\n",
    "        output['speaker'].append(speaker)\n",
    "        output['text'].append(extracted_text)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech_start': [260.0],\n",
       " 'speech_end': [950.0],\n",
       " 'speaker': ['SPEAKER_00'],\n",
       " 'text': [' en groen als']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out  = segment_text(transcription,ref_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speech_start': [260.0],\n",
       " 'speech_end': [950.0],\n",
       " 'speaker': ['SPEAKER_00'],\n",
       " 'text': [' en groen als']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tests/reference_files/text_audio_integration.json\", \"w\") as write_file:\n",
    "    json.dump(out, write_file, indent=4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('tests', 'reference_files', 'text_audio_integration.json'), 'r') as file:\n",
    "    predicted_output = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2022 16:08:02 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    }
   ],
   "source": [
    "model = AudioTextIntegrator(language='dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filepath = os.path.join('tests', 'audio_files', 'test_dutch_5_seconds.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': 'maak en groen als', 'start_timestamps': [60, 100, 180, 220, 260, 300, 380, 440, 480, 540, 580, 620, 680, 740, 800, 840, 880], 'end_timestamps': [80, 120, 200, 240, 280, 320, 400, 460, 500, 560, 600, 640, 700, 760, 820, 860, 900], 'probabilities': [0.839084267616272, 0.9999984502792358, 0.9999985694885254, 0.9999988079071045, 0.9404094815254211, 0.9997819066047668, 0.598744809627533, 0.9999990463256836, 0.9999988079071045, 0.9999977350234985, 0.9999982118606567, 0.9999430179595947, 0.999998927116394, 0.9743364453315735, 0.9999988079071045, 0.9999505281448364, 0.999866247177124]}\n",
      "[ 00:00:00.257 -->  00:00:00.949] A SPEAKER_00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_segment_text() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/evaviviani/github/mexca/audio_text_integration.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/evaviviani/github/mexca/audio_text_integration.ipynb#ch0000005?line=0'>1</a>\u001b[0m output_predicted \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mapply(audio_filepath)\n",
      "File \u001b[0;32m~/github/mexca/mexca/text/transcription.py:38\u001b[0m, in \u001b[0;36mAudioTextIntegrator.apply\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     36\u001b[0m annotation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_speech_segmenter\u001b[39m.\u001b[39mapply(filepath)\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(annotation)\n\u001b[0;32m---> 38\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_segment_text(transcription, annotation)\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mTypeError\u001b[0m: _segment_text() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "output_predicted = model.apply(audio_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = SpeakerIdentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2022 16:08:56 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    }
   ],
   "source": [
    "text_model = AudioTranscriber(language='dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_output = audio_model.apply(audio_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "text= output = text_model.apply(audio_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/evaviviani/github/mexca/audio_text_integration.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/evaviviani/github/mexca/audio_text_integration.ipynb#ch0000038?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(audio_output)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/evaviviani/github/mexca/audio_text_integration.ipynb#ch0000038?line=1'>2</a>\u001b[0m \u001b[39mlen\u001b[39m(output_predicted)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_predicted' is not defined"
     ]
    }
   ],
   "source": [
    "len(audio_output)\n",
    "len(output_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "985152c635fb7f99cf74ed94f09458d14cb4aa85491e3994d4854ecf8b71bd6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
